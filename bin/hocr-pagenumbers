#!/usr/bin/env python

import argparse
import json
import random
import re
import sys

import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from viterbi_trellis import ViterbiTrellis

from roman import fromRoman, toRoman, InvalidRomanNumeralError

from hocr.parse import hocr_page_iterator, hocr_page_to_word_data, hocr_page_get_dimensions

# TODO:
#
# - Clean up code
# - Add other numbering schemes
# - Test with logistic regression
# - Add confidence based on logistic regression and other data (maybe also word
#   confidence?)
# - Test with items in pagenumber-test-data - also the oddball ones (maybe add
#   new schemes for them, like 'arabic +' - to allow (8) as 8, etc)
# - Compare with ILL
#
# - Add different length (density_threshold) and F parameters for roman and composite sequences
#
# - Add some filter based on how far it is from the edge of the page (only 20% # maybe?)
#
# - Add some limit to composites just in case...
# - Add the ability to allow missing numbers in sequences
#
# - Maybe at the end, extrapolate outside of our regular bounds in case any
#   values are still 'None' (but with much lower confidence)
# - Add further extrapolation option (also logistic regression?)
# - Optimise code some to prevent too many back and forth conversions

# - Mention: "Versatile Page Numbering Analysis", Hervé Déjean, Jean-Luc Meunier
#
# TO HANDLE:
# * The page numbers are of the form 22/314, 23/315, 71..363 and so forth, the
#   OCR picks them up, but I am not sure if the module knows what to do with it.
# * (12)10, (12)11, (12)12
# * (7), (8), (9), etc


SKIP_NO_EDGE = True
NEGATIVES_PER_PAGE = 10

COMPOSITE_ANY_RE = re.compile(r'.*\d.*')
COMPOSITE_DIGIT_SUB = re.compile(r'\d+')
COMPOSITE_DIGIT_RE = re.compile('\d+')
COMPOSITE_SIMPLE_RES = [re.compile('^\(\d+\)'),
                        re.compile('^[A-Z]+-?\d+$'), re.compile('^\d+-?[A-Z]+$'),
                        re.compile('^\(\d+\)\(\d+\)$'), re.compile('^\(\d+\)\d+$'),
                        re.compile('^\d+/\d+$'), re.compile('^\d+\.\d+$')]


def extrapolate(scheme, sequence, start, end):
    start_pageidx, start_pagenumber = start
    end_pageidx, end_pagenumber = end

    new_sequence = []

    seqmap = dict(sequence)

    for pageidx, val in zip(range(start_pageidx, end_pageidx+1),
                            range(int(start_pagenumber.num_value), int(end_pagenumber.num_value+1))):
        if pageidx in seqmap:
            pagenumbercandidate = seqmap[pageidx]
            new_sequence.append((pageidx, pagenumbercandidate))
        else:
            newcandidate = PageNumberCandidate(scheme.from_num(val), scheme, True)
            new_sequence.append((pageidx, newcandidate))

    return new_sequence


class ArabicNumberingScheme(object):
    PAGENO_RE = re.compile(r'^\d+$')
    SUPPORT_EXTRAPOLATION = True

    def syntactic_match(self, s):
        v = ArabicNumberingScheme.PAGENO_RE.match(s)

        if v:
            try:
                int(s)
            except:
                v = False

        return v

    def is_increase(self, base, steps, check):
        return base.num_value + steps == check.num_value

    def numeral_value(self, s):
        return int(s)

    def from_num(self, n):
        return str(n)

    def extrapolate_sequence(self, sequence, start, end):
        return extrapolate(self, sequence, start, end)


class RomanNumberingScheme(object):
    SUPPORT_EXTRAPOLATION = True

    def syntactic_match(self, s):
        try:
            v = fromRoman(s)
            return True
        except InvalidRomanNumeralError:
            return False

    def is_increase(self, base, steps, check):
        return base.num_value + steps == check.num_value

    def numeral_value(self, s):
        return fromRoman(s)

    def from_num(self, n):
        return toRoman(n)

    def extrapolate_sequence(self, sequence, start, end):
        return extrapolate(self, sequence, start, end)


class SingleLetterNumberingScheme(object):
    SUPPORT_EXTRAPOLATION = True

    def syntactic_match(self, s):
        len(s) == 1 and ((s >= 'a' and s <= 'z') or
                         (s >= 'A' and s <= 'Z'))

    def is_increase(self, base, steps, check):
        return base.num_value + steps == check.num_value

    def numeral_value(self, s):
        return ord(s)

    def from_num(self, n):
        return chr(n)

    def extrapolate_sequence(self, sequence, start, end):
        return extrapolate(self, sequence, start, end)


class CompositeNumberingScheme(object):
    MUL = 10000

    def __init__(self, val):
        self.orig_val = val
        self.count = len(COMPOSITE_DIGIT_RE.findall(val))
        self.format = COMPOSITE_DIGIT_SUB.sub(r'%d', val.replace('%', '%%'))
        self.composite_scheme = '^' + COMPOSITE_DIGIT_SUB.sub(r'\\d+', re.escape(val)) + '$'
        self.comp_re = re.compile(self.composite_scheme)

        # TODO: Only do this if we deem it a 'simple' composition, like A12..A13
        # or something like that
        self.SUPPORT_EXTRAPOLATION = False

        for r in COMPOSITE_SIMPLE_RES:
            if r.match(val):
                self.SUPPORT_EXTRAPOLATION = True
                break

    def syntactic_match(self, s):
        return self.comp_re.match(s)

    def is_increase(self, base, steps, check):
        return base.num_value + steps == check.num_value

    def numeral_value(self, s):
        m = COMPOSITE_DIGIT_RE.findall(s)
        n = 0
        for idx, v in enumerate(reversed(m)):
            if idx == 0:
                n += int(v)
            else:
                n += int(v) * self.MUL**idx

        return n

    def from_num(self, n):
        idx = 0
        digits = []
        r = n % self.MUL
        if r == 0:
            digits.append(r)

        while r != 0:
            digits.append(r)
            idx += 1
            r = (n // self.MUL**idx) % self.MUL

        if len(digits) != self.count:
            digits += [0] * (self.count - len(digits))

        digits = tuple(reversed(digits))

        #print('orig:', self.orig_val)
        #print('comp:', self.composite_scheme)
        #print('format and digits:', self.format, digits)
        #print('n:', n)
        v = self.format % digits
        return v

    def extrapolate_sequence(self, sequence, start, end):
        return extrapolate(self, sequence, start, end)


def composite_factory(val):
    if COMPOSITE_ANY_RE.match(val):
        #print('New factory:', repr(val))
        #return CompositeNumberingScheme(val)
        for r in COMPOSITE_SIMPLE_RES:
            if r.match(val):
                #print('New factory:', repr(val))
                return CompositeNumberingScheme(val)
    return None


NUMBER_SCHEMES = [
    ArabicNumberingScheme(),
    RomanNumberingScheme(),
    SingleLetterNumberingScheme(),
    # Various CompositeNumberingScheme's are added on the fly
    # TODO: Add Generic ASCII family
]


class PageNumberCandidate(object):
    def __init__(self, value, scheme, synthetic, hocr=None):
        self.value = value
        self.num_value =  scheme.numeral_value(value)
        self.scheme = scheme
        self.synthetic = synthetic
        if not self.synthetic:
            self.hocr = hocr

    def __repr__(self):
        return self.value
        # TODO: properties for formal analysis / logistic regression

# positional features (14 features)
# page ratio w/h (1 feature)
# page bounding box (4 features)
# pageidx parity bit, and all above features multiplied by parity bit (20 features)
# typography: currently just font size (1 feature)
#
# = 40 features
#
# >>> inp = ['x1', 'y1', 'x2', 'y2']
# >>> mul = [(x, x) for x in inp] + list(itertools.combinations(inp, r=2))
# >>> for l, r in mul:
# ...     print('%s * %s' % (l, r))

def create_candidate_features(page_dim, page_contents_box, hocr_word, pageidx):
    # We have 41 features
    features = np.ndarray((41,), dtype=np.int32)

    # Word bbox
    x1, y1, x2, y2 = hocr_word['bbox']
    features[0:4] = [x1, y1, x2, y2]
    # Quadratic combination of word bbox
    features[4:14] = [x1 * x1, y1 * y1, x2 * x2, y2 * y2, x1 * y1, x1 * x2,
                      x1 * y2, y1 * x2, y1 * y2, x2 * y2]
    # Page ratio
    features[15] = page_dim[0] / page_dim[1]
    features[16:20] = page_contents_box
    # Page parity bit
    features[20] = 1 if pageidx % 2 == 0 else -1
    # Parity bit multiplication of previous features
    features[21:40] = features[0:19] * features[20]
    # Font size feature, multiply by 100 so we don't lose float accurate as much
    # in int (if it really ever even is a float)
    features[40] = int(hocr_word['fontsize'] * 100)

    return features


# TODO: Make saving non_matches optional, maybe dependent on filterfun being
# None, or add some pass-1, pass-2 type thing
def find_hocr_matches(hocrfile, filterfun=None):
    page_matches = []
    page_non_matches = []
    page_info = []

    composite_schemes = []

    page_stop = 10000
    for pageidx, page in enumerate(hocr_page_iterator(hocrfile)):
        page_width, page_height = hocr_page_get_dimensions(page)
        w_20 = page_width // 5
        h_20 = page_height // 5
        w_80 = page_width - w_20
        h_80 = page_height - h_20

        word_data = hocr_page_to_word_data(page)
        matches = []
        non_matches = []

        page_content_box = [0] * 4

        for par in word_data:
            for line in par['lines']:
                for word in line['words']:
                    x1, y1, x2, y2 = word['bbox']
                    page_content_box[0] = min(page_content_box[0], x1)
                    page_content_box[1] = min(page_content_box[1], y1)
                    page_content_box[2] = max(page_content_box[2], x2)
                    page_content_box[3] = max(page_content_box[3], y2)

                    if SKIP_NO_EDGE:
                        if not (x1 < w_20 or y1 < h_20 or x2 > w_80 or y2 > h_80):
                            continue


                    if filterfun:
                        if filterfun(pageidx, word):
                            continue

                    # TODO: Limit to 20% from left/right or top/bottom of page?
                    # make this a toggle?
                    text = word['text']
                    found = False
                    for scheme in NUMBER_SCHEMES + composite_schemes:
                        if scheme.syntactic_match(text):
                            match = PageNumberCandidate(text, scheme, False, word)
                            matches.append(match)
                            found = True

                        if found:
                            break

                    if found:
                        break

                    if text == '7':
                        s = ArabicNumberingScheme()
                        print(s.syntactic_match(text))
                        raise Exception(text)

                    # If we haven't matched anything, consider a new composite
                    # scheme
                    fact = composite_factory(text)
                    if fact:
                        composite_schemes.append(fact)
                        match = PageNumberCandidate(text, fact, False, word)
                        matches.append(match)
                        continue


                    # If we get to this point, the word did not match, store it
                    # for later for training purposes
                    non_matches.append(word)

        page_matches.append(matches)

        if len(non_matches):
            non_matches = random.choices(non_matches, k = NEGATIVES_PER_PAGE)

        page_non_matches.append(non_matches)

        page_info.append(([page_width, page_height], page_content_box))

        if pageidx > page_stop:
            break

    return page_matches, page_non_matches, page_info


def fits_sequence(sequence, pageidx, match):
    last_seq = sequence[-1]

    last_seq_pageidx, last_seq_val = last_seq
    step = pageidx - last_seq_pageidx

    if match.scheme.is_increase(last_seq_val, step, match) and match != last_seq_val and pageidx != last_seq_pageidx:
        return True

    return False


def greedy_sequence_enumeration(page_matches, density_threshold=0.3):
    current_sequences = []
    parked_sequences = []

    for pageidx, matches in enumerate(page_matches):
        for match in matches:
            fits = False

            for seq in current_sequences:
                if fits_sequence(seq, pageidx, match):
                    seq.append((pageidx, match))
                    fits = True
                    break

            # Create new sequence
            if not fits:
                current_sequences.append([(pageidx, match)])

        # Figure out which sequences to park (so that we don't have to take them
        # into account in the analysis going forward)
        park_idx = []
        for idx, sequence in enumerate(current_sequences):
            seq_len = len(sequence)

            seq_start = sequence[0][0]
            seq_tail = pageidx

            seq_diff = seq_tail - seq_start
            density = seq_len / (seq_tail - seq_start) if seq_diff != 0 else 1

            if density < density_threshold:
                move_seq = current_sequences.pop(idx)
                parked_sequences.append(move_seq)

    # Park remaining sequences
    for sequence in current_sequences:
        parked_sequences.append(sequence)
    current_sequences = []

    # Filter 1-length out
    parked_sequences = list(filter(lambda x: len(x) > 1, parked_sequences))

    return parked_sequences


class State(object):
    def __init__(self, pageidx, value):
        self.pageidx = pageidx
        self.value = value
        self.links = {}

    def link(self, other_state, cost):
        self.links[other_state] = cost

    def get_cost(self, other_state):
        try:
            return self.links[other_state]
        except:
            # TODO: Investigate this, we really want the cost to be defined for
            # everything
            #print('Failed for page idx %d with value %s and page idx %d with value %s' % (self.pageidx, self.value, other_state.pageidx, other_state.value))
            return 2.

    def __repr__(self):
        return '(%s, %s)' % (self.pageidx, self.value)

        #s = '%s -> [' % self.value
        #vals = list(map(lambda x: str(x.value) if x.value else 'None', self.links))
        #s += ','.join(vals)
        #s += ']'
        #return s


def create_graph(sequences, page_count, F=3):
    # XXX: This requires the gaps in sequences to be filled up

    graph = []

    NONE_COST = 1. # Max cost

    page_states = {}
    for page in range(page_count):
        page_states[page] = [State(page, None)]

    for sequence in sequences:
        N = len(sequence)
        sequence_states = []

        for idx, (pageidx, value) in enumerate(sequence):
            s = State(pageidx, value)
            if (idx > 0):
                cost = 1 - (F / N)
                # XXX: The algorithm we work with currently uses cost, not score, so we add 1 - ... here
                sequence_states[idx-1].link(s, 1 - cost)

                #sequence_states[idx-1].link(s, cost)

            sequence_states.append(s)
            page_states[pageidx].append(s)

    # Now connect all the states of the previous layer to the None state of the
    # next layer and connect all the None states to the other sequences
    for page in range(1, page_count):
        prev_page_states = page_states[page - 1]
        current_none_state = page_states[page][0]

        # XXX: It should also be possible for sequences to jump from the end of
        # one sequence to another, the code does not allow for this yet
        # unfortunately

        for prev_page_state in prev_page_states:
            # TODO: We might want some of these to be negative if a state is
            # leaving its sequence early
            prev_page_state.link(current_none_state, NONE_COST)

        prev_page_none_state = prev_page_states[0]
        for page_state in page_states[page]:
            prev_page_none_state.link(page_state, NONE_COST)



    list_page_states = []
    for page in range(page_count):
        list_page_states.append(page_states[page])

    return list_page_states


def fill_holes(sequences):
    new_sequences = []

    for sequence in sequences:
        scheme = sequence[0][1].scheme
        if scheme.SUPPORT_EXTRAPOLATION:
            start_pageidx = sequence[0][0]
            end_pageidx = sequence[-1][0]
            start_val = sequence[0][1]
            end_val = sequence[-1][1]

            new_sequence = scheme.extrapolate_sequence(sequence,
                                                       (start_pageidx, start_val),
                                                       (end_pageidx, end_val))
            new_sequences.append(new_sequence)
        else:
            new_sequences.append(sequence)

    return new_sequences


def candidates_from_trellis(best_path, trellis):
    results = []

    for page_idx, (choice_idx, vals) in enumerate(zip(best_path, trellis)):
        results.append((page_idx, vals[choice_idx].value))

    return results


def train_model(candidates, page_non_matches, pages_info):
    positive_features = []
    negative_features = []

    for page_idx, candidate in candidates:
        page_info = pages_info[page_idx]
        if candidate and not candidate.synthetic:
            feature = create_candidate_features(page_info[0], page_info[1],
                                                candidate.hocr, page_idx)
            positive_features.append(feature)

            # Create (max, if we can) 10 negative samples
            for non_match in page_non_matches[page_idx]:
                feature = create_candidate_features(page_info[0], page_info[1],
                                                    non_match, page_idx)
                negative_features.append(feature)

    total_features = len(positive_features) + len(negative_features)

    X = np.ndarray((total_features, 41), dtype=np.int32)
    X[0:len(positive_features)] = np.array(positive_features)
    X[len(positive_features):] = np.array(negative_features)

    y = np.array([1.] * len(positive_features) + [0.] * len(negative_features))

    scaler = StandardScaler()
    X_ = scaler.fit_transform(X)
    #y = scaler.transform(y)

    lr = LogisticRegression(C=1.0, solver='liblinear')
    clf = lr.fit(X_, y)

    #features = positive_features + negative_features
    #cnt = 0
    #for feature in positive_features:
    #    npf = np.array(feature).reshape((1, 41))
    #    npf = scaler.transform(npf)
    #    v = clf.predict(npf)
    #    if v > 0.:
    #        cnt += 1

    #print('P Total:', len(positive_features), 'correct:', cnt)

    #cnt = 0
    #for feature in negative_features:
    #    npf = np.array(feature).reshape((1, 41))
    #    npf = scaler.transform(npf)
    #    v = clf.predict(npf)
    #    if v <= 0.:
    #        cnt += 1
    #    #print('N', v)
    #print('N Total:', len(negative_features), 'correct:', cnt)

    #for page_idx, candidate in candidates:
    #    page_info = pages_info[page_idx]
    #    if candidate and not candidate.synthetic:
    #        feature = create_candidate_features(page_info[0], page_info[1],
    #                                            candidate.hocr, page_idx)
    #        npf = np.array(feature).reshape((1, 41))
    #        npf = scaler.transform(npf)
    #        print(clf.predict_log_proba(npf))
    #        print(clf.predict_proba(npf))
    #        v = clf.predict(npf)
    #        if v > 0:
    #            print('Accept;', candidate.value)
    #        else:
    #            print('Reject:', candidate.value)

    return clf, scaler



def pagenos_from_trellis(best_path, trellis):
    pagenos = []

    last_state = None
    for page_idx, (choice_idx, vals) in enumerate(zip(best_path, trellis)):
#        #print('\tpicked: %s -> %s' % (last_state, vals[choice_idx]))
#        #print()
#
#        if last_state is not None:
#            #print('New choices:',)
#            for link in last_state.links:
#                pass
#                #print(last_state, '->', link, ' with cost', last_state.get_cost(link))
#
#
        last_state = vals[choice_idx]

        # +1 for 1-based page numbering just so that we can understand it in our
        # debug prints
        pagenos.append((page_idx, last_state.value))

    return pagenos


def create_page_json(pagenos):
    data = {'pages': [], 'confidence': 'todo', 'identifier': 'todo'}

    for (pageidx, pagenoval) in pagenos:
        # TODO: add pagenumber as string, and then also another interpretation
        # in case it is a composite page number?
        data['pages'].append({'leafNum': pageidx,
                              'pageNumber': str(pagenoval.value) if pagenoval else ''})
                              #'pageNumber': pagenoval.value if pagenoval else ''})

    return data


def process_file(hocrfile, outfile, two_pass=True):
    # Step 1: find candidates for each page (with coords)
    page_matches, page_non_matches, page_info = find_hocr_matches(hocrfile)

    #from pprint import pprint
    #pprint(page_matches)

    sequences = greedy_sequence_enumeration(page_matches)
    ##pprint(sequences)

    # XXX: Let's fill holes later
    if not two_pass:
        sequences = fill_holes(sequences)
    #pprint(sequences)

    trellis = create_graph(sequences, len(page_matches))
    #pprint(trellis)

    v = ViterbiTrellis(trellis, lambda x: 1., lambda x, y: x.get_cost(y))
    best_path = v.viterbi_best_path()
    #print(best_path)

    if two_pass:
        candidates = candidates_from_trellis(best_path, trellis)

        lg_classifier, lg_scaler = train_model(candidates, page_non_matches, page_info)


        def prediction_filter(page_idx, hocr):
            pinfo = page_info[page_idx]
            feature = create_candidate_features(pinfo[0], pinfo[1],
                                                hocr, page_idx)
            npf = np.array(feature).reshape((1, 41))
            npf = lg_scaler.transform(npf)
            v = lg_classifier.predict(npf)
            # Negative if v <= 0
            return v <= 0

        page_matches, _, _ = find_hocr_matches(hocrfile, filterfun=prediction_filter)

        sequences = greedy_sequence_enumeration(page_matches,
                                                  density_threshold=0.3)
        sequences = fill_holes(sequences)
        trellis = create_graph(sequences, len(page_matches), F=1)
        v = ViterbiTrellis(trellis, lambda x: 1., lambda x, y: x.get_cost(y))
        best_path = v.viterbi_best_path()

    pagenos = pagenos_from_trellis(best_path, trellis)


    # Assign features
    ...

    # TODO: Formal analysis with logistic regression

    page_json = create_page_json(pagenos)
    outfile_fp = open(outfile, 'w')
    json.dump(page_json, outfile_fp, indent=' ' * 4)
    outfile_fp.close()


    #debug(page_matches)




if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='hOCR to plaintext')
    parser.add_argument('-f', '--infile', help='Filename to read',
                        type=str, default=None)
    parser.add_argument('-o', '--outfile', help='Filename to write to',
                        type=str, default=None)
    args = parser.parse_args()

    process_file(args.infile, args.outfile)
